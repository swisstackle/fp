{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b69e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06ae6dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_pose=mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "609a8077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8966d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    #mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION)           # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)               # Draw pose connections\n",
    "    #mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)          # Draw left hand connections\n",
    "    #mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b58a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "   \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a9f1945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aef9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.array(['good left half','good right half','bad left half', 'bad right half'])\n",
    "sq_len=20\n",
    "#fps_a= cap.get(cv2.CAP_PROP_FPS)\n",
    "#frame_count=cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "#no_sq=round(frame_count/fps_a)\n",
    "no_sq=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3367ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_q=['lr.mp4','rl1.mp4','rl2.mp4']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f0bf84",
   "metadata": {},
   "source": [
    "# LEFT RIGHT GOOD DATA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d074325",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('Trainingvideosfinal/good/vid1/lr.mp4')\n",
    "# used to record the time when we processed last frame\n",
    "prev_frame_time = 0\n",
    "\n",
    "# used to record the time at which we processed current frame\n",
    "new_frame_time = 0\n",
    "# VIDEO FEED\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    slope_shoulder_l,slope_hip_l,rate_slope_shoulder_l,rate_slope_shoulder_length_l=[],[],[],[]\n",
    "\n",
    "    differenceHipsList = []\n",
    "    differenceShouldersList = []\n",
    "    leftshoulderList = []\n",
    "    rightshoulderList = []\n",
    "    lsp=[]\n",
    "    rsp=[]\n",
    "    lhp=[]\n",
    "    rhp=[]\n",
    "    lefthipList = []\n",
    "    righthipList = []\n",
    "    midpointsholder_l=[]\n",
    "    midpointhip_l=[]\n",
    "    spinelength_l=[]\n",
    "    s_len=[]\n",
    "    h_len=[]\n",
    "    ratio_l=[]\n",
    "    framecounter = 0\n",
    "    while cap.isOpened():\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame is not None:\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            image.flags.writeable = False\n",
    "            results = pose.process(image)\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            new_frame_time = time.time()\n",
    "        # Calculating the fps\n",
    "\n",
    "        # fps will be number of frame processed in given time frame\n",
    "        # since their will be most of time error of 0.001 second\n",
    "        # we will be subtracting it to get more accurate result\n",
    "            fps = 1 / (new_frame_time - prev_frame_time)\n",
    "            prev_frame_time = new_frame_time\n",
    "\n",
    "        # converting the fps into integer\n",
    "            fps = int(fps)\n",
    "\n",
    "        # converting the fps to string so that we can display it on frame\n",
    "        # by using putText function\n",
    "            fps_1 = str(fps)\n",
    "\n",
    "        # putting the FPS count on the frame\n",
    "            cv2.putText(image, fps_1, (7, 70), font, 3, (100, 255, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                leftshoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "                rightshoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
    "                differenceShoulders = np.absolute(rightshoulder.z - leftshoulder.z)\n",
    "                righthip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value]\n",
    "                lefthip = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value]\n",
    "                differenceHips = np.absolute(righthip.z - lefthip.z)\n",
    "                differenceHipsList.append(differenceHips)\n",
    "                differenceShouldersList.append(differenceShoulders)\n",
    "                leftshoulderList.append([leftshoulder.x, leftshoulder.y])\n",
    "                rightshoulderList.append([rightshoulder.x, rightshoulder.y])\n",
    "                lefthipList.append([lefthip.x, lefthip.y])\n",
    "                righthipList.append([righthip.x, righthip.y])\n",
    "\n",
    "            except:\n",
    "\n",
    "                pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            differenceHipsAverage = np.mean(differenceHipsList)\n",
    "            differenceShouldersAverage = np.mean(differenceShouldersList)\n",
    "\n",
    "\n",
    "            slope_shoulder = np.arctan((leftshoulderList[framecounter][1] - rightshoulderList[framecounter][1]) / (leftshoulderList[framecounter][0] - rightshoulderList[framecounter][0]))\n",
    "            slope_hip = np.arctan((lefthipList[framecounter][1] - righthipList[framecounter][1]) / (lefthipList[framecounter][0] - righthipList[framecounter][0]))\n",
    "\n",
    "            rate_slope_shoulder = slope_shoulder * fps\n",
    "\n",
    "            rate_slope_shoulder_length = differenceShouldersAverage * fps\n",
    "            midpointsholder_p_x = (leftshoulder.x+rightshoulder.x)/2\n",
    "            midpointsholder_p_y = (leftshoulder.y+rightshoulder.y)/2\n",
    "            leftsholder_poin=np.array([leftshoulder.x,leftshoulder.y])\n",
    "            rightsholder_poin=np.array([rightshoulder.x,rightshoulder.y])\n",
    "            lefthip_poin=np.array([lefthip.x,lefthip.y])\n",
    "            righthip_poin=np.array([righthip.x,righthip.y])\n",
    "            midpointsholder_poin=np.array([midpointsholder_p_x,midpointsholder_p_y])\n",
    "            midpointhip_p_x = (lefthip.x+righthip.x)/2\n",
    "            midpointhip_p_y = (lefthip.y+righthip.y)/2\n",
    "            midpointhip_poin=np.array([midpointhip_p_x,midpointhip_p_y])\n",
    "\n",
    "            s_len_v=np.linalg.norm(leftsholder_poin-rightsholder_poin)\n",
    "            h_len_v=np.linalg.norm(lefthip_poin-righthip_poin)\n",
    "            ratio_v=s_len_v/h_len_v\n",
    "            spinelength_v=np.linalg.norm(midpointsholder_poin-midpointhip_poin)\n",
    "            framecounter += 1\n",
    "\n",
    "\n",
    "\n",
    "        #To solve shoulder fluctuation problem with being stuck on one side, we should be able to detect in which phase of the gait cycle the subject is in each frame.\n",
    "        #In order to determine phase of gait cycle, we have to know the neutral position (standing neutral position) and the corresponding bones positions.\n",
    "        #From there, we should be able to detect in which phase the subject is in each frame.\n",
    "\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                    mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                                    mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "                                    )\n",
    "\n",
    "\n",
    "            cv2.imshow('Mediapipe Feed', image)\n",
    "            #image_l.append(image)\n",
    "            #print(image.shape)\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "            lsp.append([leftshoulder.x,leftshoulder.y]),rsp.append([rightshoulder.x,rightshoulder.y]),lhp.append([lefthip.x,lefthip.y]),rhp.append([righthip.x,righthip.y]),ratio_l.append(ratio_v),spinelength_l.append(spinelength_v),midpointsholder_l.append([midpointsholder_p_x,midpointsholder_p_y]),midpointhip_l.append([midpointhip_p_x,midpointhip_p_y]),slope_shoulder_l.append(slope_shoulder),slope_hip_l.append(slope_hip),rate_slope_shoulder_l.append(rate_slope_shoulder),rate_slope_shoulder_length_l.append(rate_slope_shoulder_length),s_len.append(s_len_v),h_len.append(h_len_v)\n",
    "\n",
    "cap.release()\n",
    "#print(midpointhip_l)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9114b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'difference_shoulder': differenceShouldersList,\n",
    "      'difference_hip': differenceHipsList,\n",
    "      'Spine_Length':spinelength_l,\n",
    "      'Shoulder_Length':s_len,\n",
    "      'Slope_shoulder':slope_shoulder_l,\n",
    "      'Slope_hip':slope_hip_l\n",
    "      \n",
    "        }\n",
    "dataF12=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1273f90",
   "metadata": {},
   "source": [
    "# RIGHT LEFT GOOD DATA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e417803d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('Trainingvideosfinal/good/vid1/rl1.mp4')\n",
    "# used to record the time when we processed last frame\n",
    "prev_frame_time = 0\n",
    "\n",
    "# used to record the time at which we processed current frame\n",
    "new_frame_time = 0\n",
    "# VIDEO FEED\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    slope_shoulder_l,slope_hip_l,rate_slope_shoulder_l,rate_slope_shoulder_length_l=[],[],[],[]\n",
    "\n",
    "    differenceHipsList = []\n",
    "    differenceShouldersList = []\n",
    "    leftshoulderList = []\n",
    "    rightshoulderList = []\n",
    "    lsp=[]\n",
    "    rsp=[]\n",
    "    lhp=[]\n",
    "    rhp=[]\n",
    "    lefthipList = []\n",
    "    righthipList = []\n",
    "    midpointsholder_l=[]\n",
    "    midpointhip_l=[]\n",
    "    spinelength_l=[]\n",
    "    s_len=[]\n",
    "    h_len=[]\n",
    "    ratio_l=[]\n",
    "    framecounter = 0\n",
    "    while cap.isOpened():\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame is not None:\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            image.flags.writeable = False\n",
    "            results = pose.process(image)\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            new_frame_time = time.time()\n",
    "        # Calculating the fps\n",
    "\n",
    "        # fps will be number of frame processed in given time frame\n",
    "        # since their will be most of time error of 0.001 second\n",
    "        # we will be subtracting it to get more accurate result\n",
    "            fps = 1 / (new_frame_time - prev_frame_time)\n",
    "            prev_frame_time = new_frame_time\n",
    "\n",
    "        # converting the fps into integer\n",
    "            fps = int(fps)\n",
    "\n",
    "        # converting the fps to string so that we can display it on frame\n",
    "        # by using putText function\n",
    "            fps_1 = str(fps)\n",
    "\n",
    "        # putting the FPS count on the frame\n",
    "            cv2.putText(image, fps_1, (7, 70), font, 3, (100, 255, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                leftshoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "                rightshoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
    "                differenceShoulders = np.linalg.norm(rightshoulder.z - leftshoulder.z)\n",
    "                righthip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value]\n",
    "                lefthip = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value]\n",
    "                differenceHips = np.linalg.norm(righthip.z - lefthip.z)\n",
    "                differenceHipsList.append(differenceHips)\n",
    "                differenceShouldersList.append(differenceShoulders)\n",
    "                leftshoulderList.append([leftshoulder.x, leftshoulder.y])\n",
    "                rightshoulderList.append([rightshoulder.x, rightshoulder.y])\n",
    "                lefthipList.append([lefthip.x, lefthip.y])\n",
    "                righthipList.append([righthip.x, righthip.y])\n",
    "\n",
    "            except:\n",
    "\n",
    "                pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            differenceHipsAverage = np.mean(differenceHipsList)\n",
    "            differenceShouldersAverage = np.mean(differenceShouldersList)\n",
    "\n",
    "\n",
    "            slope_shoulder = np.arctan((leftshoulderList[framecounter][1] - rightshoulderList[framecounter][1]) / (leftshoulderList[framecounter][0] - rightshoulderList[framecounter][0]))\n",
    "            slope_hip = np.arctan((lefthipList[framecounter][1] - righthipList[framecounter][1]) / (lefthipList[framecounter][0] - righthipList[framecounter][0]))\n",
    "\n",
    "            rate_slope_shoulder = slope_shoulder * fps\n",
    "\n",
    "            rate_slope_shoulder_length = differenceShouldersAverage * fps\n",
    "            midpointsholder_p_x = (leftshoulder.x+rightshoulder.x)/2\n",
    "            midpointsholder_p_y = (leftshoulder.y+rightshoulder.y)/2\n",
    "            leftsholder_poin=np.array([leftshoulder.x,leftshoulder.y])\n",
    "            rightsholder_poin=np.array([rightshoulder.x,rightshoulder.y])\n",
    "            lefthip_poin=np.array([lefthip.x,lefthip.y])\n",
    "            righthip_poin=np.array([righthip.x,righthip.y])\n",
    "            midpointsholder_poin=np.array([midpointsholder_p_x,midpointsholder_p_y])\n",
    "            midpointhip_p_x = (lefthip.x+righthip.x)/2\n",
    "            midpointhip_p_y = (lefthip.y+righthip.y)/2\n",
    "            midpointhip_poin=np.array([midpointhip_p_x,midpointhip_p_y])\n",
    "\n",
    "            s_len_v=np.linalg.norm(leftsholder_poin-rightsholder_poin)\n",
    "            h_len_v=np.linalg.norm(lefthip_poin-righthip_poin)\n",
    "            ratio_v=s_len_v/h_len_v\n",
    "            spinelength_v=np.linalg.norm(midpointsholder_poin-midpointhip_poin)\n",
    "            framecounter += 1\n",
    "\n",
    "\n",
    "\n",
    "        #To solve shoulder fluctuation problem with being stuck on one side, we should be able to detect in which phase of the gait cycle the subject is in each frame.\n",
    "        #In order to determine phase of gait cycle, we have to know the neutral position (standing neutral position) and the corresponding bones positions.\n",
    "        #From there, we should be able to detect in which phase the subject is in each frame.\n",
    "\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                    mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                                    mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "                                    )\n",
    "\n",
    "\n",
    "            cv2.imshow('Mediapipe Feed', image)\n",
    "            #image_l.append(image)\n",
    "            #print(image.shape)\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "            lsp.append([leftshoulder.x,leftshoulder.y]),rsp.append([rightshoulder.x,rightshoulder.y]),lhp.append([lefthip.x,lefthip.y]),rhp.append([righthip.x,righthip.y]),ratio_l.append(ratio_v),spinelength_l.append(spinelength_v),midpointsholder_l.append([midpointsholder_p_x,midpointsholder_p_y]),midpointhip_l.append([midpointhip_p_x,midpointhip_p_y]),slope_shoulder_l.append(slope_shoulder),slope_hip_l.append(slope_hip),rate_slope_shoulder_l.append(rate_slope_shoulder),rate_slope_shoulder_length_l.append(rate_slope_shoulder_length),s_len.append(s_len_v),h_len.append(h_len_v)\n",
    "\n",
    "cap.release()\n",
    "#print(midpointhip_l)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d94f93dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('Trainingvideosfinal/good/vid1/rl2.mp4')\n",
    "# used to record the time when we processed last frame\n",
    "prev_frame_time = 0\n",
    "\n",
    "# used to record the time at which we processed current frame\n",
    "new_frame_time = 0\n",
    "# VIDEO FEED\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "\n",
    "    framecounter = 0\n",
    "    while cap.isOpened():\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame is not None:\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            image.flags.writeable = False\n",
    "            results = pose.process(image)\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            new_frame_time = time.time()\n",
    "        # Calculating the fps\n",
    "\n",
    "        # fps will be number of frame processed in given time frame\n",
    "        # since their will be most of time error of 0.001 second\n",
    "        # we will be subtracting it to get more accurate result\n",
    "            fps = 1 / (new_frame_time - prev_frame_time)\n",
    "            prev_frame_time = new_frame_time\n",
    "\n",
    "        # converting the fps into integer\n",
    "            fps = int(fps)\n",
    "\n",
    "        # converting the fps to string so that we can display it on frame\n",
    "        # by using putText function\n",
    "            fps_1 = str(fps)\n",
    "\n",
    "        # putting the FPS count on the frame\n",
    "            cv2.putText(image, fps_1, (7, 70), font, 3, (100, 255, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                leftshoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "                rightshoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
    "                differenceShoulders = np.absolute(rightshoulder.z - leftshoulder.z)\n",
    "                righthip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value]\n",
    "                lefthip = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value]\n",
    "                differenceHips = np.absolute(righthip.z - lefthip.z)\n",
    "                differenceHipsList.append(differenceHips)\n",
    "                differenceShouldersList.append(differenceShoulders)\n",
    "                leftshoulderList.append([leftshoulder.x, leftshoulder.y])\n",
    "                rightshoulderList.append([rightshoulder.x, rightshoulder.y])\n",
    "                lefthipList.append([lefthip.x, lefthip.y])\n",
    "                righthipList.append([righthip.x, righthip.y])\n",
    "\n",
    "            except:\n",
    "\n",
    "                pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            differenceHipsAverage = np.mean(differenceHipsList)\n",
    "            differenceShouldersAverage = np.mean(differenceShouldersList)\n",
    "\n",
    "\n",
    "            slope_shoulder = np.arctan((leftshoulderList[framecounter][1] - rightshoulderList[framecounter][1]) / (leftshoulderList[framecounter][0] - rightshoulderList[framecounter][0]))\n",
    "            slope_hip = np.arctan((lefthipList[framecounter][1] - righthipList[framecounter][1]) / (lefthipList[framecounter][0] - righthipList[framecounter][0]))\n",
    "\n",
    "            rate_slope_shoulder = slope_shoulder * fps\n",
    "\n",
    "            rate_slope_shoulder_length = differenceShouldersAverage * fps\n",
    "            midpointsholder_p_x = (leftshoulder.x+rightshoulder.x)/2\n",
    "            midpointsholder_p_y = (leftshoulder.y+rightshoulder.y)/2\n",
    "            leftsholder_poin=np.array([leftshoulder.x,leftshoulder.y])\n",
    "            rightsholder_poin=np.array([rightshoulder.x,rightshoulder.y])\n",
    "            lefthip_poin=np.array([lefthip.x,lefthip.y])\n",
    "            righthip_poin=np.array([righthip.x,righthip.y])\n",
    "            midpointsholder_poin=np.array([midpointsholder_p_x,midpointsholder_p_y])\n",
    "            midpointhip_p_x = (lefthip.x+righthip.x)/2\n",
    "            midpointhip_p_y = (lefthip.y+righthip.y)/2\n",
    "            midpointhip_poin=np.array([midpointhip_p_x,midpointhip_p_y])\n",
    "\n",
    "            s_len_v=np.linalg.norm(leftsholder_poin-rightsholder_poin)\n",
    "            h_len_v=np.linalg.norm(lefthip_poin-righthip_poin)\n",
    "            ratio_v=s_len_v/h_len_v\n",
    "            spinelength_v=np.linalg.norm(midpointsholder_poin-midpointhip_poin)\n",
    "            framecounter += 1\n",
    "\n",
    "\n",
    "\n",
    "        #To solve shoulder fluctuation problem with being stuck on one side, we should be able to detect in which phase of the gait cycle the subject is in each frame.\n",
    "        #In order to determine phase of gait cycle, we have to know the neutral position (standing neutral position) and the corresponding bones positions.\n",
    "        #From there, we should be able to detect in which phase the subject is in each frame.\n",
    "\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                    mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                                    mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "                                    )\n",
    "\n",
    "\n",
    "            cv2.imshow('Mediapipe Feed', image)\n",
    "            #image_l.append(image)\n",
    "            #print(image.shape)\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "            lsp.append([leftshoulder.x,leftshoulder.y]),rsp.append([rightshoulder.x,rightshoulder.y])\n",
    "            lhp.append([lefthip.x,lefthip.y])\n",
    "            rhp.append([righthip.x,righthip.y])\n",
    "            ratio_l.append(ratio_v)\n",
    "            spinelength_l.append(spinelength_v)\n",
    "            midpointsholder_l.append([midpointsholder_p_x,midpointsholder_p_y])\n",
    "            midpointhip_l.append([midpointhip_p_x,midpointhip_p_y])\n",
    "            slope_shoulder_l.append(slope_shoulder)\n",
    "            slope_hip_l.append(slope_hip)\n",
    "            rate_slope_shoulder_l.append(rate_slope_shoulder)\n",
    "            rate_slope_shoulder_length_l.append(rate_slope_shoulder_length)\n",
    "            s_len.append(s_len_v),h_len.append(h_len_v)\n",
    "\n",
    "cap.release()\n",
    "#print(midpointhip_l)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "615d0d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data={'difference_shoulder': differenceShouldersList,\n",
    "      'difference_hip': differenceHipsList,\n",
    "      'Spine_Length':spinelength_l,\n",
    "      'Shoulder_Length':s_len,\n",
    "      'Slope_shoulder':slope_shoulder_l,\n",
    "      'Slope_hip':slope_hip_l\n",
    "        }\n",
    "dataF22=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c537d901",
   "metadata": {},
   "source": [
    "# LEFT RIGHT BAD DATA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbdf2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'difference_shoulder': differenceShouldersList,\n",
    "      'difference_hip': differenceHipsList,\n",
    "      'Spine_Length':spinelength_l,\n",
    "      'Shoulder_Length':s_len,\n",
    "      'Slope_shoulder':slope_shoulder_l,\n",
    "      'Slope_hip':slope_hip_l\n",
    "      \n",
    "        }\n",
    "dataF32=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102597ca",
   "metadata": {},
   "source": [
    "# RIGHT LEFT BAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e0d47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'difference_shoulder': differenceShouldersList,\n",
    "      'difference_hip': differenceHipsList,\n",
    "      'Spine_Length':spinelength_l,\n",
    "      'Shoulder_Length':s_len,\n",
    "      'Slope_shoulder':slope_shoulder_l,\n",
    "      'Slope_hip':slope_hip_l\n",
    "      \n",
    "        }\n",
    "dataF42=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd3750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b994c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(out)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10e204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=label_map.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ed8250",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFR = [np.array(dataF12),np.array(dataF22),np.array(dataF32),np.array(dataF42)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3489f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq, labels =[], []\n",
    "for i in range(len(out)):\n",
    "    seq.append(dataFR[i])\n",
    "    seq[i]=seq[i][:30]\n",
    "    labels.append(label_map[out[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b1c9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq=np.array(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84832313",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(seq)\n",
    "y = to_categorical(labels).astype(int)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, shuffle=True, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac57506",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a5a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58d62ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_dir = os.path.join('Logs')\n",
    "#tb_callback = EarlyStopping(monitor='val_loss', patience=30, mode='min', restore_best_weights=True)\n",
    "#e_callback = TensorBoard(log_dir=log_dir)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2becace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,6)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "# model.add(Dropout(0.12))\n",
    "# model.add(LSTM(256,return_sequences=True, activation='relu'))  #added layer\n",
    "# model.add(LSTM(128, return_sequences=True, activation='relu')) #added layer\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(out.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d0e7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b773a2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='RMSProp', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006788f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=1000, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c0bbcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
